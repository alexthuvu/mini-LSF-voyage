---
title: Mini LSF Voyage
emoji: ğŸŒ
colorFrom: blue
colorTo: indigo
app_file: app.py
pinned: false
license: cc-by-nc-4.0
short_description: Learn French Sign Language with live camera feedback ğŸ“¸âœ‹
---

**Learn French Sign Language through live camera feedback â€” small dataset, big journey.**

Built from scratch using custom video recordings, Mediapipe keypoints, and a Conv1D Transformer-based gesture recognition model. This app runs in your browser with real-time predictions powered by a webcam.

---

## ğŸ“– Story of the Build

> This whole thing started as a crazy ideaâ€¦ and became a 4-week deep dive into sign recognition.

### ğŸ—“ 2025.05.24 â€” Day 1

- started project with zero dataset, just webcam and a will
- first goal: record and preprocess LSF sign clips (Bonjour, Ã‡a va, Boire, Quoi, Au revoir)

### ğŸ“¹ Week 1: Data Recording & First Failures

- edited and processed raw videos into keypoints using MediaPipe
- trained first models using LSTM and GRU with just 24 samples per class (lol)
- tried thick models with tiny dropout, nothing worked
- validation accuracy stuck around 0.11
- kept shrinking signs: 5 â†’ 2 signs, still no good results
- lesson: more data needed â€” model wasnâ€™t the problem, data was

### ğŸ“ˆ Week 2: Real Dataset Expansion

- spent 3 days recording new samples with different angles, clothes, lighting (even masks and hoodies)
- processed everything again
- model reached 0.58 val_acc max with bidirectional LSTMs and normalization tricks
- couldnâ€™t break the ceiling, almost gave up
- AI said: â€œjust get more dataâ€ â€” I had no more

### ğŸ§  Week 3: Transformers Saved Me

- first time trying Conv1D + Transformer encoder â€” hit 0.71 val_acc
- no overfitting, stable
- prediction code sucked at first â€” camera predictions failed even with strong model
- reduced to 2-class set, model hit 0.91 val_acc, loss < 0.05
- problem: double-preprocessing + missing facial keypoints

### ğŸ”§ Week 4: Real Fixes, Real Results

- fixed input bug (removed double-preprocess)
- added full face keypoints
- chose final 5 signs that could make a short story
- final model: Conv1D + Transformer layers + residual + batchnorm + dropout
- finally got accurate live prediction working in Gradio

---

## ğŸ§ª How It Works

- Live webcam frame â†’ MediaPipe â†’ landmark sequence (pose + face + hands)
- Sequence (64 frames) â†’ Conv1D-Transformer-based model â†’ predicted gesture
- Smart validation rules handle signs based on finger positions & face distances
- If model confidence is high and matches expected sign â†’ advance to next scene

---

## ğŸ“ Tech Stack

- Python 3.10, TensorFlow / Keras
- Flask + JS + HTML
- DOCKER helpscontainer
- MediaPipe Holistic
- All signs recorded manually (me + family + a few ext videos on youtube ğŸ˜‚)

---

## ğŸ“¦ Dataset

Small, handcrafted dataset of 5 LSF signs:

- About 40 - 60 samples per sign
- Multiple backgrounds, outfits, angles
- Converted to 1629-feature keypoint sequences

---

## ğŸš€ Run It

Clone the repo, install requirements, then launch:

```bash
pip install -r requirements.txt
python app.py
```
